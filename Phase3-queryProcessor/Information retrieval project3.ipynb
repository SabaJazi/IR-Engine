{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ElementTree\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import os \n",
    "import itertools\n",
    "#from collections import OrderedDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_unique = []\n",
    "docno_dicts = {} \n",
    "#-----------reading parser output-------------------\n",
    "word_list={}\n",
    "\n",
    "with open('parser_output.txt', 'r') as f:   \n",
    "    for line in f:\n",
    "        (key, val) = line.split()\n",
    "        word_list[key] = val\n",
    "\n",
    "#-----------------------------------reading stop word file to a list----------------------------------\n",
    "\n",
    "stop_w=[]\n",
    "\n",
    "with open('stopwordlist.txt', 'r') as f:\n",
    "    stop_w = f.read().split()\n",
    " \n",
    "\n",
    " #-----------------------------------the function that reads each document file----------------------------------\n",
    "\n",
    "def Indexer(filename): \n",
    "    with open(filename, 'r') as f:   \n",
    "        xml = f.read()\n",
    "        \n",
    "    xml = '<ROOT>' + xml + '</ROOT>'   # adding a root tag\n",
    "    root = ElementTree.fromstring(xml)\n",
    "    \n",
    " #------------------------------counting number of documents-----------------------------------------    \n",
    "    doc_count=0\n",
    "    \n",
    "    for doc in root:\n",
    "        doc_no=doc.find('DOCNO').text.strip()\n",
    "        doc_count=doc_count+1   \n",
    "        \n",
    "        \n",
    "    \n",
    "#------------------------------counting occurance of each word in each document----------   \n",
    "    \n",
    "    for doc in root:\n",
    "          \n",
    "        doc_no=doc.find('DOCNO').text.strip()\n",
    "        \n",
    "        text_string=doc.find('TEXT').text.strip()\n",
    "        text_string=text_string.lower() \n",
    "        text_string= re.findall(r'\\w+', text_string) \n",
    "        text_string = [ele for ele in text_string if ele not in stop_w ]\n",
    "        text_string = [ele for ele in text_string if not any(c.isdigit() for c in ele)]\n",
    "        \n",
    "        dictionary = {}\n",
    "        #-------------------------porter stemmer to stem and remove duplicates----------\n",
    "        for elements in text_string: \n",
    "            ps = PorterStemmer() \n",
    "            elements=ps.stem(elements)\n",
    "            if elements in word_list:\n",
    "                         word_id=int(word_list[elements])\n",
    "        #-------------------------making dictionary of frequency of each word in doc-----+df----   \n",
    "            \n",
    "            if word_id in inverted_index.keys():\n",
    "                if doc_no in inverted_index[word_id].keys():\n",
    "                    inverted_index[word_id][int(word_list[doc_no])] += 1\n",
    "                else: \n",
    "                    inverted_index[word_id].update({int(word_list[doc_no]): 1})            \n",
    "                    df_dict[word_id]=+1\n",
    "                  \n",
    "                        \n",
    "            else:\n",
    "                inverted_index[word_id] = {} \n",
    "                inverted_index[word_id][int(word_list[doc_no])]= 1\n",
    "                df_dict[word_id]= 1 \n",
    "            \n",
    "            if word_id in dictionary.keys(): \n",
    "                dictionary[word_id]+= 1\n",
    "            else: \n",
    "               \n",
    "                dictionary[word_id]= 1 \n",
    "                \n",
    "        #------------making dictionary of dictionaries of each doc---------------  \n",
    "        \n",
    "        forward_index[doc_no] = dictionary \n",
    "        #-------------------------making tf--------------\n",
    "       \n",
    "       # for key in forward_index:\n",
    "        #tf_dict[key]=dictionary[key]/float(len(dictionary)) \n",
    "    #-------------now all the documents are processed and forward+ invert+df are complete--------\n",
    "    for d_num in forward_index.keys():\n",
    "        tf_idf_dict[word_list[d_num]]={}\n",
    "        for w_id in forward_index[d_num].keys():\n",
    "            tf_idf_dict[word_list[d_num]][w_id]=float(round((forward_index[d_num][w_id])*(np.log((doc_count)/df_dict[w_id])),2))\n",
    "            \n",
    "    \n",
    "            \n",
    "    \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "desc=[]\n",
    "narr=[]\n",
    "\n",
    "\n",
    " #-----------------------------------the function that reads each query file----------------------------------\n",
    "\n",
    "def q_Indexer(filename): \n",
    "    with open(filename, 'r') as f:   \n",
    "        xml = f.read()\n",
    "   \n",
    "    result=xml.split(\"<top>\")\n",
    "    q_count=len(result)\n",
    "    #--------making a dictionary of query numbers and their ids-----------------------------\n",
    "    for i in range(1,len(result)):\n",
    "\n",
    "        _void, str1=result[i].split(\"<num> Number:\")\n",
    "        q_num, str1=str1.split(\"<title>\")\n",
    "        q_num=int(q_num.strip())\n",
    "        q_num_id[q_num]=i  \n",
    "        q_num_id2[q_num]=i  \n",
    "        q_num_id3[q_num]=i  \n",
    "    #------------------------------------------------\n",
    "    for i in range(1,len(result)):\n",
    "        \n",
    "        _void, str1=result[i].split(\"<num> Number:\")\n",
    "        q_num, str1=str1.split(\"<title>\")\n",
    "        q_num=int(q_num.strip())\n",
    "        __title, str1=str1.split(r\"<desc> Description:\")\n",
    "        __desc, str1=str1.split(\"<narr> Narrative: \")\n",
    "        __narr,__void =str1.split(\"</top>\")\n",
    "        \n",
    "        text_string2=__title +__desc\n",
    "        text_string3=__title + __narr\n",
    "        \n",
    "        text_string=__title.lower() \n",
    "        text_string2=text_string2.lower() \n",
    "        text_string3=text_string3.lower() \n",
    "        \n",
    "        \n",
    "        text_string= re.findall(r'\\w+', text_string) \n",
    "        text_string2= re.findall(r'\\w+', text_string2) \n",
    "        text_string3= re.findall(r'\\w+', text_string3) \n",
    "        \n",
    "        text_string = [ele for ele in text_string if ele not in stop_w ]\n",
    "        text_string2 = [ele for ele in text_string2 if ele not in stop_w ]\n",
    "        text_string3 = [ele for ele in text_string3 if ele not in stop_w ]\n",
    "        \n",
    "        text_string = [ele for ele in text_string if not any(c.isdigit() for c in ele)]\n",
    "        text_string2 = [ele for ele in text_string2 if not any(c.isdigit() for c in ele)]\n",
    "        text_string3 = [ele for ele in text_string3 if not any(c.isdigit() for c in ele)]\n",
    "        \n",
    "            \n",
    "#------------------------------counting occurance of each word in each query----------   \n",
    "        \n",
    "        dictionary = {}\n",
    "        #-------------------------porter stemmer to stem and remove duplicates----------\n",
    "        for elements in text_string: \n",
    "            ps = PorterStemmer() \n",
    "            elements=ps.stem(elements)\n",
    "            if elements in word_list:\n",
    "                         word_id=int(word_list[elements])\n",
    "        #-------------------------making dictionary of frequency of each word in doc-----+df----   \n",
    "            \n",
    "            if word_id in q_inverted_index.keys():\n",
    "                if q_num in q_inverted_index[word_id].keys():\n",
    "                    q_inverted_index[word_id][int(q_num_id[q_num])] += 1\n",
    "                else: \n",
    "                    q_inverted_index[word_id].update({int(q_num_id[q_num]): 1})            \n",
    "                    q_df_dict[word_id]=+1\n",
    "                  \n",
    "                        \n",
    "            else:\n",
    "                q_inverted_index[word_id] = {} \n",
    "                q_inverted_index[word_id][int(q_num_id[q_num])]= 1\n",
    "                q_df_dict[word_id]= 1 \n",
    "            \n",
    "            if word_id in dictionary.keys(): \n",
    "                dictionary[word_id]+= 1\n",
    "            else: \n",
    "               \n",
    "                dictionary[word_id]= 1 \n",
    "                \n",
    "        #------------making dictionary of dictionaries of each query---------------  \n",
    "        \n",
    "        q_forward_index[q_num] = dictionary \n",
    "        \n",
    "        #------------------------------option 2----------------\n",
    "        dictionary2 = {}\n",
    "        for elements in text_string2: \n",
    "            ps = PorterStemmer() \n",
    "            elements=ps.stem(elements)\n",
    "            if elements in word_list:\n",
    "                         word_id=int(word_list[elements])\n",
    "        #-------------------------making dictionary of frequency of each word in doc-----+df----   \n",
    "            \n",
    "            if word_id in q_inverted_index2.keys():\n",
    "                if q_num in q_inverted_index2[word_id].keys():\n",
    "                    q_inverted_index2[word_id][int(q_num_id2[q_num])] += 1\n",
    "                else: \n",
    "                    q_inverted_index2[word_id].update({int(q_num_id2[q_num]): 1})            \n",
    "                    q_df_dict2[word_id]=+1\n",
    "                  \n",
    "                        \n",
    "            else:\n",
    "                q_inverted_index2[word_id] = {} \n",
    "                q_inverted_index2[word_id][int(q_num_id2[q_num])]= 1\n",
    "                q_df_dict2[word_id]= 1 \n",
    "            \n",
    "            if word_id in dictionary2.keys(): \n",
    "                dictionary2[word_id]+= 1\n",
    "            else: \n",
    "               \n",
    "                dictionary2[word_id]= 1 \n",
    "                \n",
    "        #------------making dictionary of dictionaries of each query---------------  \n",
    "        \n",
    "        q_forward_index2[q_num] = dictionary2\n",
    "        \n",
    "        #------------------------------option 3----------------\n",
    "        dictionary3 = {}\n",
    "        for elements in text_string3: \n",
    "            ps = PorterStemmer() \n",
    "            elements=ps.stem(elements)\n",
    "            if elements in word_list:\n",
    "                         word_id=int(word_list[elements])\n",
    "        #-------------------------making dictionary of frequency of each word in doc-----+df----   \n",
    "            \n",
    "            if word_id in q_inverted_index3.keys():\n",
    "                if q_num in q_inverted_index3[word_id].keys():\n",
    "                    q_inverted_index3[word_id][int(q_num_id3[q_num])] += 1\n",
    "                else: \n",
    "                    q_inverted_index3[word_id].update({int(q_num_id3[q_num]): 1})            \n",
    "                    q_df_dict3[word_id]=+1\n",
    "                  \n",
    "                        \n",
    "            else:\n",
    "                q_inverted_index3[word_id] = {} \n",
    "                q_inverted_index3[word_id][int(q_num_id3[q_num])]= 1\n",
    "                q_df_dict3[word_id]= 1 \n",
    "            \n",
    "            if word_id in dictionary3.keys(): \n",
    "                dictionary3[word_id]+= 1\n",
    "            else: \n",
    "               \n",
    "                dictionary3[word_id]= 1 \n",
    "                \n",
    "        #------------making dictionary of dictionaries of each query---------------  \n",
    "        \n",
    "        q_forward_index3[q_num] = dictionary3\n",
    "        \n",
    "        \n",
    "    #-------------now all the documents are processed and forward+ invert+df are complete--------\n",
    "    for q_num in q_forward_index.keys():\n",
    "        q_tf_idf_dict[q_num_id[q_num]]={}\n",
    "        for w_id in q_forward_index[q_num].keys():\n",
    "            q_tf_idf_dict[q_num_id[q_num]][w_id]=float(round((q_forward_index[q_num][w_id])*(np.log((q_count)/q_df_dict[w_id])),2))\n",
    "            \n",
    "   #-------------now all the documents are processed and forward+ invert+df are complete--------\n",
    "    for q_num in q_forward_index2.keys():\n",
    "        q_tf_idf_dict2[q_num_id2[q_num]]={}\n",
    "        for w_id in q_forward_index2[q_num].keys():\n",
    "            q_tf_idf_dict2[q_num_id2[q_num]][w_id]=float(round((q_forward_index2[q_num][w_id])*(np.log((q_count)/q_df_dict2[w_id])),2))\n",
    "            \n",
    "     \n",
    "  #-------------now all the documents are processed and forward+ invert+df are complete--------\n",
    "    for q_num in q_forward_index3.keys():\n",
    "        q_tf_idf_dict3[q_num_id3[q_num]]={}\n",
    "        for w_id in q_forward_index3[q_num].keys():\n",
    "            q_tf_idf_dict3[q_num_id3[q_num]][w_id]=float(round((q_forward_index3[q_num][w_id])*(np.log((q_count)/q_df_dict3[w_id])),2))\n",
    "                       \n",
    "    \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------reading, processing and calculating the query tf_idf-----------------------\n",
    "Q_DIR = './Query-Documents/'\n",
    "\n",
    "q_forward_index ={}\n",
    "q_forward_index2 ={}\n",
    "q_forward_index3 ={}\n",
    "\n",
    "q_inverted_index={}\n",
    "q_inverted_index2={}\n",
    "q_inverted_index3={}\n",
    "\n",
    "q_num_id={}\n",
    "q_num_id2={}\n",
    "q_num_id3={}\n",
    "\n",
    "q_df_dict={}\n",
    "q_df_dict2={}\n",
    "q_df_dict3={}\n",
    "\n",
    "q_tf_idf_dict={}\n",
    "q_tf_idf_dict2={}\n",
    "q_tf_idf_dict3={}\n",
    "\n",
    "for file_name in os.listdir(Q_DIR):\n",
    "    q_Indexer(Q_DIR + file_name)\n",
    "    \n",
    "with open('q_forward_index.txt', 'w') as f:    \n",
    "    for key, value in q_forward_index.items():\n",
    "        print(q_num_id[key],'\\t',str(value)[1:-1] ,'\\n',   file=f) \n",
    "        \n",
    "with open('q_forward_index2.txt', 'w') as f:    \n",
    "    for key, value in q_forward_index2.items():\n",
    "        print(q_num_id2[key],'\\t',str(value)[1:-1] ,'\\n',   file=f) \n",
    "        \n",
    "with open('q_forward_index3.txt', 'w') as f:    \n",
    "    for key, value in q_forward_index3.items():\n",
    "        print(q_num_id3[key],'\\t',str(value)[1:-1] ,'\\n',   file=f) \n",
    "        \n",
    "        \n",
    "        \n",
    "with open('q_inverted_index.txt', 'w') as f:    \n",
    "    for key, value in q_inverted_index.items():\n",
    "        print(key,'\\t',str(value)[1:-1] ,'\\n',   file=f)\n",
    "\n",
    "with open('q_inverted_index2.txt', 'w') as f:    \n",
    "    for key, value in q_inverted_index2.items():\n",
    "        print(key,'\\t',str(value)[1:-1] ,'\\n',   file=f)\n",
    "        \n",
    "with open('q_inverted_index3.txt', 'w') as f:    \n",
    "    for key, value in q_inverted_index3.items():\n",
    "        print(key,'\\t',str(value)[1:-1] ,'\\n',   file=f)\n",
    " \n",
    "\n",
    "with open('q_tf_idf_index.txt', 'w') as f:  \n",
    "    for key, value in q_tf_idf_dict.items():\n",
    "        print(key,'\\t',str(value)[1:-1] ,'\\n',   file=f)\n",
    "        \n",
    "with open('q_tf_idf_index2.txt', 'w') as f:  \n",
    "    for key, value in q_tf_idf_dict2.items():\n",
    "        print(key,'\\t',str(value)[1:-1] ,'\\n',   file=f)\n",
    "        \n",
    "with open('q_tf_idf_index3.txt', 'w') as f:  \n",
    "    for key, value in q_tf_idf_dict3.items():\n",
    "        print(key,'\\t',str(value)[1:-1] ,'\\n',   file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------making forward and inverted index of documents----------------------------\n",
    "\n",
    "DOC_DIR = './IR-Documents/'\n",
    "\n",
    "forward_index ={}\n",
    "inverted_index={}\n",
    "\n",
    "df_dict={}\n",
    "tf_idf_dict={}\n",
    "for file_name in os.listdir(DOC_DIR):\n",
    "    Indexer(DOC_DIR + file_name)\n",
    "    \n",
    "with open('forward_index.txt', 'w') as f:    \n",
    "    for key, value in forward_index.items():\n",
    "        print(word_list[key],'\\t',str(value)[1:-1] ,'\\n',   file=f) \n",
    "        \n",
    "with open('inverted_index.txt', 'w') as f:    \n",
    "    for key, value in inverted_index.items():\n",
    "        print(key,'\\t',str(value)[1:-1] ,'\\n',   file=f)\n",
    " \n",
    "\n",
    "with open('tf_idf_index.txt', 'w') as f:  \n",
    "    for key, value in tf_idf_dict.items():\n",
    "        print(key,'\\t',str(value)[1:-1] ,'\\n',   file=f)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#---------------------making the vector for all of the words of all of the documents---------------------------\n",
    "#N is number of total number of documents\n",
    "#N=len(forward_index)\n",
    "N=376\n",
    "# total_vocab_size is lenght inverted_index\n",
    "\n",
    "#total_vocab_size=len(inverted_index)\n",
    "\n",
    "total_vocab_size=33183\n",
    "\n",
    "\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "D = D.astype(object)\n",
    "\n",
    "for doc_id in tf_idf_dict:\n",
    "    \n",
    "    for key,value in tf_idf_dict[doc_id].items():\n",
    "        #print(key)\n",
    "        D[int(doc_id)][int(key)] = float(value)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(query_dict):\n",
    "   # print(query_dict)\n",
    "\n",
    "    Q = np.zeros(total_vocab_size)\n",
    "    Q=Q.astype(object)\n",
    "    \n",
    "    words_count = len(q_tf_idf_dict)\n",
    "    \n",
    "    for key,value in query_dict.items():\n",
    "        #print(key)\n",
    "        Q[int(key)] = float(value)\n",
    "    return Q\n",
    "\n",
    "def gen_vector2(query_dict2):\n",
    "   # print(query_dict2)\n",
    "\n",
    "    Q1 = np.zeros(total_vocab_size)\n",
    "    Q1=Q1.astype(object)\n",
    "    \n",
    "    words_count = len(q_tf_idf_dict2)\n",
    "    \n",
    "    for key,value in query_dict2.items():\n",
    "        #print(key)\n",
    "        Q1[int(key)] = float(value)\n",
    "    return Q1\n",
    "\n",
    "def gen_vector3(query_dict3):\n",
    "\n",
    "    Q2 = np.zeros(total_vocab_size)\n",
    "    Q2=Q2.astype(object)\n",
    "    \n",
    "    words_count = len(q_tf_idf_dict3)\n",
    "    \n",
    "    for key,value in query_dict3.items():\n",
    "        #print(key)\n",
    "        Q2[int(key)] = float(value)\n",
    "        \n",
    "    return Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#------------------------------------------------\n",
    "def cosine_sim(a, b):\n",
    "    chek=(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    if chek!=0.0:\n",
    "        cos_sim =round( np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b)),5)\n",
    "    else:\n",
    "        cos_sim=0\n",
    "        \n",
    "    return cos_sim\n",
    "\n",
    "#----------------------------------by default calculates the title cosin similarity----------------------\n",
    "def cosine_similarity(n):\n",
    "    d_cosines = []\n",
    "    d_cosin_dict={}\n",
    "    for key, value in q_tf_idf_dict.items(): \n",
    "        cosin_sorted[key]={}\n",
    "        #print(value)\n",
    "        query_vector = gen_vector(value)   \n",
    "        i=1\n",
    "        for d_id in D:\n",
    "            cosin_=cosine_sim(query_vector, d_id)\n",
    "            d_cosines.append(cosin_)\n",
    "            if cosin_!=0.0:\n",
    "                cosin_sorted[key][i]=cosin_\n",
    "            i+=1\n",
    "        out = np.array(d_cosines).argsort()[-n:][::-1]\n",
    "        \n",
    "        return(out)\n",
    "    \n",
    "def cosine_similarity2(n):\n",
    "    d_cosines2 = []\n",
    "    d_cosin_dict2={}\n",
    "    for key, value in q_tf_idf_dict2.items():\n",
    "        cosin_sorted2[key]={}\n",
    "        #print(value)\n",
    "        query_vector2 = gen_vector2(value)   \n",
    "        i=1\n",
    "        for d_id in D:\n",
    "            cosin2_=cosine_sim(query_vector2, d_id)\n",
    "            d_cosines2.append(cosin2_)\n",
    "            if cosin2_!=0.0 :\n",
    "                cosin_sorted2[key][i]=cosin2_\n",
    "            i+=1\n",
    "        out2 = np.array(d_cosines2).argsort()[-n:][::-1]\n",
    "\n",
    "        return(out2)\n",
    "\n",
    "    \n",
    "def cosine_similarity3(n):\n",
    "    d_cosines3 = []\n",
    "    d_cosin_dict3={}\n",
    "    for key, value in q_tf_idf_dict3.items(): \n",
    "        cosin_sorted3[key]={}\n",
    "        #print(value)\n",
    "        query_vector3 = gen_vector3(value)   \n",
    "        i=1\n",
    "        for d_id in D:\n",
    "            cosin3_=cosine_sim(query_vector3, d_id)\n",
    "            d_cosines3.append(cosin3_)\n",
    "            if cosin3_!=0.0:\n",
    "                cosin_sorted3[key][i]=cosin3_\n",
    "            i+=1\n",
    "        out3 = np.array(d_cosines3).argsort()[-n:][::-1]\n",
    "\n",
    "        return(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------cosin similarity running---------------------------------\n",
    "\n",
    "cosin_sorted={}\n",
    "cosin_sorted2={}\n",
    "cosin_sorted3={}\n",
    "\n",
    "Q = cosine_similarity(5) \n",
    "#print(Q)\n",
    "Q2 = cosine_similarity2(5) \n",
    "Q3 = cosine_similarity3(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cosin_sorted={k: v for k, v in sorted(cosin_sorted.items(), key=lambda item: item[1],reverse=True)}\n",
    "#print((cosin_sorted))\n",
    "\n",
    "q_num_dict={1:352, 2:353, 3:354, 4:359}\n",
    "#x = itertools.islice(cosin_sorted.items, 0, 5)\n",
    "\n",
    "with open('title_output.txt', 'w') as f:  \n",
    "    for q_n, value in cosin_sorted.items():\n",
    "       \n",
    "        value={k: v for k, v in sorted(value.items(), key=lambda item: item[1],reverse=True)}\n",
    "        for key, val in value.items():\n",
    "            print(q_num_dict[q_n],'\\t',\"FTP911-\"+str(key),'\\t',str(val) ,'\\n',   file=f)\n",
    "             #print(q_num_id2[key],'\\t',str(value)[1:-1] ,'\\n',   file=f) \n",
    "            \n",
    "          #  print(q_num_dict[q_n],'\\t',\"FTP911-\"+str(key),'\\t',\"0\"+str(value) ,'\\n',   file=f)\n",
    "\n",
    " #--------------------------------------------------------------       \n",
    "#x = itertools.islice(cosin_sorted.items, 0, 5)\n",
    "\n",
    "with open('title&description _output.txt', 'w') as f:  \n",
    "    for q_n, value in cosin_sorted2.items():\n",
    "        value={k: v for k, v in sorted(value.items(), key=lambda item: item[1],reverse=True)}\n",
    "        for key, val in value.items():\n",
    "            print(q_num_dict[q_n],'\\t',\"FTP911-\"+str(key),'\\t',str(val) ,'\\n',   file=f)\n",
    "             #print(q_num_id2[key],'\\t',str(value)[1:-1] ,'\\n',   file=f) \n",
    "\n",
    "          #  print(q_num_dict[q_n],'\\t',\"FTP911-\"+str(key),'\\t',\"0\"+str(value) ,'\\n',   file=f)\n",
    "        \n",
    "        \n",
    "        \n",
    "with open('title&narrative _output.txt', 'w') as f:  \n",
    "    for q_n, value in cosin_sorted3.items():\n",
    "        value={k: v for k, v in sorted(value.items(), key=lambda item: item[1],reverse=True)}\n",
    "        for key, val in value.items():\n",
    "            print(q_num_dict[q_n],'\\t',\"FTP911-\"+str(key),'\\t',str(val) ,'\\n',   file=f)\n",
    "             #print(q_num_id2[key],'\\t',str(value)[1:-1] ,'\\n',   file=f) \n",
    "\n",
    "          #  print(q_num_dict[q_n],'\\t',\"FTP911-\"+str(key),'\\t',\"0\"+str(value) ,'\\n',   file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
